{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"23-24_problem_logs.csv\")\n",
    "df['start_time'] = pd.to_datetime(df['start_time'], utc = True)\n",
    "df = df[['user_xid', 'old_problem_id', 'skill_id', 'discrete_score', 'start_time']]\n",
    "df = df.sort_values(by = ['user_xid', 'start_time'])\n",
    "problemEncoder =   LabelEncoder()\n",
    "skillEncoder = LabelEncoder()\n",
    "\n",
    "df['encoded_problem_id'] = problemEncoder.fit_transform(df['old_problem_id'])\n",
    "df['encoded_skill_id'] = skillEncoder.fit_transform(df['skill_id'])\n",
    "\n",
    "max_encoded_value = max(df['encoded_problem_id'].max(), df['encoded_skill_id'].max())\n",
    "grouped_data = df.groupby('user_xid').apply(\n",
    "    lambda x: list(zip(x['encoded_problem_id'], x['encoded_skill_id'], x['discrete_score']))\n",
    ").reset_index(name = 'problem_skill_score')\n",
    "\n",
    "\n",
    "def process_data(grouped_data: pd.DataFrame, num_steps: int) -> list:\n",
    "    users_data = []\n",
    "    for _, row in grouped_data.iterrows():\n",
    "        seq = row['problem_skill_score']\n",
    "        pids = [p[0] for p in seq]\n",
    "        sids = [p[1] for p in seq]\n",
    "        scores = [p[2] for p in seq]\n",
    "\n",
    "        if len(pids) > num_steps:\n",
    "            pids = pids[:num_steps]\n",
    "            sids = sids[:num_steps]\n",
    "            scores = scores[:num_steps]\n",
    "        else:\n",
    "            padding = [0] * (num_steps - len(pids))\n",
    "            pids.extend(padding)\n",
    "            sids.extend(padding)\n",
    "            scores.extend(padding)\n",
    "        users_data.append((pids, sids, scores))\n",
    "\n",
    "    return users_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 50\n",
    "NUM_SKILLS = max_encoded_value - 1\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.2\n",
    "NUM_HEADS = 8\n",
    "TRAIN_DATA = process_data(grouped_data, NUM_STEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAKTModel(tf.keras.Model):\n",
    "    def __init__(self, num_skills, num_steps, hidden_units, dropout_rate, num_heads, batch_size):\n",
    "        super(SAKTModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skills = num_skills\n",
    "        self.num_steps = num_steps,\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_heads = num_heads\n",
    "        self.enc_embedding = tf.keras.layers.Embedding(input_dim = num_skills * 2, output_dim = hidden_units)\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(input_dim = num_steps, output_dum = hidden_units)\n",
    "        self.multihead_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = hidden_units)\n",
    "        self.feed_forward = tf.keras.Sequential([tf.keras.layers.Dense(2048, activation = 'relu'),\n",
    "                                                 tf.keras.layers.Dense(self.hidden_units)])\n",
    "        self.sigmoid_w = tf.keras.layers.Dense(num_skills, use_bias = True)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate = dropout_rate)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_2 = 0.98)\n",
    "    def call(self, inputs, training = False):\n",
    "        x, problems, target_id ,target_correctness = inputs\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        key_masks = tf.cast(tf.not_equal(x, 0), tf.float32)\n",
    "        enc = self.enc_embedding(x) + self.pos_embedding(tf.range(self.num_steps - 1))\n",
    "        enc = enc * tf.expand_dims(key_masks, axis = 1)\n",
    "        enc = self.dropout(enc, training = training)\n",
    "        attn_output = self.multihead_attention(enc, enc, training = training)\n",
    "        attn_output += enc\n",
    "        attn_output = tf.keras.layers.LayerNormalization()(attn_output)\n",
    "        output = self.feed_forward(attn_output)\n",
    "        output += attn_output\n",
    "        output = tf.keras.layers.LayerNormalization()(output)\n",
    "        output = tf.reshape(output, [-1, self.hidden_units])\n",
    "        logits = self.sigmoid_w(output)\n",
    "        logits = tf.reshape(logits, [-1])\n",
    "        selected_logits = tf.gather(logits, target_id)\n",
    "        pred = tf.sigmoid(selected_logits)\n",
    "        target_correctness = tf.cast(target_correctness, tf.float32)\n",
    "        loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=selected_logits, labels=target_correctness))\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred, loss = self.call(inputs, training = True)\n",
    "    grads = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, data, num_skills, num_steps, batch_size, is_training):\n",
    "    actual = []\n",
    "    pred = []\n",
    "    idx = 0\n",
    "\n",
    "    with tqdm(total= len(data)//batch_size if batch_size else len(data)) as pbar:\n",
    "        while idx + batch_size < len(data):\n",
    "            x = np.zeros((batch_size, num_steps-1))\n",
    "            problems = np.zeros((batch_size, num_steps -1))\n",
    "            target_id = []\n",
    "            target_correct = []\n",
    "\n",
    "            for i in range (batch_size):\n",
    "                problem_ids, skill_ids, correctness = data[idx + i]\n",
    "                for j in range (num_steps -1):\n",
    "                    problem_id = int(problem_ids[j])\n",
    "                    label_idx = problem_id + (num_skills if int(correctness[j]) else 0)\n",
    "                    x [i, j] = label_idx\n",
    "                    problems[i, j]  = problem_ids[j + 1] if j + 1 < num_steps else 0\n",
    "                    target_id.append(i * (num_steps-1) + j)\n",
    "                    target_correct.append(correctness[j + 1] if j + 1 < num_steps else 0)\n",
    "                    actual.append(correctness[j+1] if j + 1 < num_steps else 0)\n",
    "            idx += batch_size\n",
    "\n",
    "            inputs = (x, problems, target_id, target_correct)\n",
    "            predict, _ = model.call(inputs, training = is_training)\n",
    "\n",
    "            pred.extend(pred.numpy())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(actual, pred))\n",
    "    fpr, tpr, _ = metrics.roc_curve(actual, pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return rmse, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_data = process_data(grouped_data, NUM_STEPS)\n",
    "    model = SAKTModel(num_skills=NUM_SKILLS, num_steps=NUM_STEPS, hidden_units=HIDDEN_UNITS, dropout_rate=DROPOUT_RATE, num_heads=NUM_HEADS)\n",
    "    for epoch in range (0, 10):\n",
    "        print (f'\\nEpoch{epoch} / 5')\n",
    "        rmse, auc = run_epoch(model, TRAIN_DATA, NUM_SKILLS, NUM_STEPS, is_training= True)\n",
    "        print(f'Epoch: {epoch}, Train RMSE: {rmse:.3f}, Train AUC: {auc:.3f}')\n",
    "        if epoch % 5 == 0:\n",
    "            rmse, auc = run_epoch(model, train_data, NUM_SKILLS, NUM_STEPS, is_training=False)\n",
    "            print(f'Epoch: {epoch}, Test RMSE: {rmse:.3f}, Test AUC: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
