{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Masking\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('23-24-problem_logs.csv') # fill with data file\n",
    "#'23-24-problem_logs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 213. GiB for an array with shape (54437, 1048575) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# One-hot encode 'skill_id' and 'old_problem_id'\u001b[39;00m\n\u001b[0;32m      6\u001b[0m skill_ohe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskill_id\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskill\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m problem_ohe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mold_problem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproblem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Combine the encoded columns back into the DataFrame\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data_cleaned \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_cleaned, skill_ohe, problem_ohe], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:221\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    219\u001b[0m     result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:330\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     eye_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[1;32m--> 330\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meye_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     dummy_mat[codes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 213. GiB for an array with shape (54437, 1048575) and data type int32"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "df[['skill_id', 'old_problem_id']] = df[['skill_id', 'old_problem_id']].apply(pd.to_numeric, errors='coerce')\n",
    "data_cleaned = df.fillna(0)  # Fill missing values with 0 for now\n",
    "\n",
    "# One-hot encode 'skill_id' and 'old_problem_id'\n",
    "skill_ohe = pd.get_dummies(data_cleaned['skill_id'], dtype=int, prefix='skill')\n",
    "problem_ohe = pd.get_dummies(data_cleaned['old_problem_id'], dtype=int, prefix='problem')\n",
    "\n",
    "# Combine the encoded columns back into the DataFrame\n",
    "data_cleaned = pd.concat([data_cleaned, skill_ohe, problem_ohe], axis=1)\n",
    "\n",
    "# Remove the original 'skill_id' and 'old_problem_id' columns\n",
    "data_cleaned.drop(['skill_id', 'old_problem_id'], axis=1, inplace=True)\n",
    "\n",
    "# Group by 'user_xid'\n",
    "grouped = data_cleaned.groupby('user_xid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(511, 571)\n"
     ]
    }
   ],
   "source": [
    "# data_cleaned.head()\n",
    "print(data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 173 at dim 2 (got 391)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m [[skill_row, problem_row] \u001b[38;5;28;01mfor\u001b[39;00m skill_row, problem_row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(skill_feature_seq, problem_feature_seq)]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Append the combined features (list of lists) to the feature sequence\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m feature_seq\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Ensure labels are treated as a tensor, even if they are single values\u001b[39;00m\n\u001b[0;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscrete_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 173 at dim 2 (got 391)"
     ]
    }
   ],
   "source": [
    "seq = []\n",
    "lab = []\n",
    "for user, group in grouped:\n",
    "    group = group.sort_values(by='start_time')\n",
    "\n",
    "    skill_feature_seq = group[skill_ohe.columns].to_numpy()  # One-hot encoded skill features\n",
    "    problem_feature_seq = group[problem_ohe.columns].to_numpy()  # One-hot encoded problem features\n",
    "\n",
    "    # Create a combined list of arrays: [[skill_ohe], [problem_ohe]]\n",
    "    feature_seq = [zip(skill_feature_seq, problem_feature_seq)]\n",
    "\n",
    "    seq.append(torch.tensor(feature_seq, dtype=torch.float32))\n",
    "    \n",
    "    # Ensure labels are treated as a tensor, even if they are single values\n",
    "    labels = torch.tensor(group['discrete_score'].to_numpy(), dtype=torch.float32)\n",
    "    lab.append(labels)\n",
    "\n",
    "# Numeber of attempts vary, so padding is required, padding with 0\n",
    "padded_seq = pad_sequences([s.numpy() for s in seq], padding='post', dtype='float32')\n",
    "padded_lab = pad_sequences([l.numpy() for l in lab], padding='post', dtype='float32') \n",
    "\n",
    "# Reshape labels to match output format (1 for each timestep)\n",
    "padded_lab = padded_lab.reshape(padded_lab.shape[0], padded_lab.shape[1], 1)  # Ensure 3D shape (batch_size, timesteps, 1)\n",
    "\n",
    "\n",
    "# Masking\n",
    "# Assuming `skill_id` is at index 0 and `problem_id` is at index 1 in the feature vector\n",
    "mask = np.logical_not((padded_seq[:, :, 0] == 0) & (padded_seq[:, :, 1] == 0))  # True where both are 0\n",
    "\n",
    "# Apply the mask to the features and labels\n",
    "masked_padded_seq = padded_seq * mask[:, :, None]  # Apply mask to all features\n",
    "masked_padded_lab = padded_lab * mask[:, :, None]  # Apply mask to labels as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_seq, padded_lab, test_size=0.2, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Instantiation\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(None, 2)))  # Mask rows where all features are 0 (after masking)\n",
    "model.add(LSTM(64, input_shape=(None, 2), activation='tanh', return_sequences=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(X_train, y_train, \n",
    "         validation_data=(X_test, y_test),\n",
    "         epochs=10, \n",
    "         batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Test Data\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Test Data\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
