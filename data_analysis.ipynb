{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import scipy.stats as stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "years = ['19-20','20-21','21-22','22-23','23-24']\n",
    "models = ['DKT', 'SAKT']\n",
    "fold_nums = range(5)\n",
    "sample_nums = range(1,11)"
   ],
   "id": "4bdbb136cabca42f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f = open('./Data/dkt_results/cy_DKT_19-20_1.json')\n",
    "j = json.load(f)\n",
    "f.close()"
   ],
   "id": "f09cd87ebe75c460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "j",
   "id": "cbbcde9b153fb4ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dkt_wy_dict = {}\n",
    "dkt_cy_dict = {}\n",
    "for y in years: # DKT\n",
    "\tdkt_wy_dict[y] = []\n",
    "\tfor f in fold_nums:\n",
    "\t\twith open(f'./Data/dkt_results/wy_DKT_{y}_{f}.json') as wyj:\n",
    "\t\t\tobj = json.load(wyj)\n",
    "\t\t\tdkt_wy_dict[y].append(obj[y][str(f)])\n",
    "\tif y != '23-24':\n",
    "\t\tdkt_cy_dict[y] = {}\n",
    "\t\tfor s in sample_nums:\n",
    "\t\t\twith open(f'./Data/dkt_results/cy_DKT_{y}_{s}.json') as cyj:\n",
    "\t\t\t\tobj = json.load(cyj)\n",
    "\t\t\t\tfor test_year, measurement in obj.items():\n",
    "\t\t\t\t\tif dkt_cy_dict[y].get(test_year) is None:\n",
    "\t\t\t\t\t\tdkt_cy_dict[y][test_year] = {}\n",
    "\t\t\t\t\tdkt_cy_dict[y][test_year][str(s)] = measurement"
   ],
   "id": "24f83033709a7022",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sakt_wy_dict = {}\n",
    "sakt_cy_dict = {}\n",
    "for y in years: # SAKT\n",
    "\tsakt_wy_dict[y] = []\n",
    "\tfor f in fold_nums:\n",
    "\t\twith open(f'./Data/sakt_results/wy_SAKT_{y}_{f}.json') as wyj:\n",
    "\t\t\tobj = json.load(wyj)\n",
    "\t\t\tsakt_wy_dict[y].append(obj[y][str(f)])\n",
    "\tif y != '23-24':\n",
    "\t\tsakt_cy_dict[y] = {}\n",
    "\t\tfor s in sample_nums:\n",
    "\t\t\twith open(f'./Data/sakt_results/cy_SAKT_{y}_{s}.json') as cyj:\n",
    "\t\t\t\tobj = json.load(cyj)\n",
    "\t\t\t\tfor test_year, measurement in obj.items():\n",
    "\t\t\t\t\tif sakt_cy_dict[y].get(test_year) is None:\n",
    "\t\t\t\t\t\tsakt_cy_dict[y][test_year] = {}\n",
    "\t\t\t\t\tsakt_cy_dict[y][test_year][str(s)] = measurement"
   ],
   "id": "79c4e41aa885f218",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('./Data/formatted_results/cross_year_results_DKT.json','w') as f:\n",
    "\tjson.dump(dkt_cy_dict,f)"
   ],
   "id": "20e5ca2cf14b1b3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('./Data/formatted_results/cross_year_results_SAKT.json','w') as f:\n",
    "\tjson.dump(sakt_cy_dict,f)"
   ],
   "id": "3a5dbd733dbb4589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('./Data/formatted_results/within_year_results_DKT.json','w') as f:\n",
    "\tjson.dump(dkt_wy_dict,f)"
   ],
   "id": "9ba48246887a5116",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('./Data/formatted_results/within_year_results_SAKT.json','w') as f:\n",
    "\tjson.dump(sakt_wy_dict,f)"
   ],
   "id": "c3c0e904e3824f48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "st = \"./Data/formatted_results/within_year_results_SAKT.json\"\n",
    "st.split('_')[-1].split('.')[0]"
   ],
   "id": "f7ad77d87a155587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dir = './Data/formatted_results'\n",
    "mean_dict = {}\n",
    "res_dict = {}\n",
    "for fn in os.listdir(dir):\n",
    "\tf = os.path.join(dir,fn)\n",
    "\tmodel = fn.split('_')[-1].split('.')[0]\n",
    "\twith open(f,'r') as jf:\n",
    "\t\tobj = json.load(jf)\n",
    "\tif mean_dict.get(model) is None:\n",
    "\t\tmean_dict[model] = {}\n",
    "\t\tres_dict[model] = {}\n",
    "\t\tprint(f'initializing {model}')\n",
    "\tif 'cross' in fn:\n",
    "\t\t# print(mean_dict)\n",
    "\t\tfor train_y, dc in obj.items():\n",
    "\t\t\tfor test_year, res in dc.items():\n",
    "\t\t\t\tif mean_dict[model].get(test_year) is None:\n",
    "\t\t\t\t\t# print(f'Initializing {model} {train_y}')\n",
    "\t\t\t\t\tmean_dict[model][test_year] = {}\n",
    "\t\t\t\t\tres_dict[model][test_year] = {}\n",
    "\t\t\t\tdf = pd.DataFrame(res.values())\n",
    "\t\t\t\tstatsd = df.describe()\n",
    "\t\t\t\tmean_dict[model][test_year][train_y] = statsd[0]['mean']\n",
    "\t\t\t\tres_dict[model][test_year][train_y] =list(res.values())\n",
    "\telse:\n",
    "\t\t# print(mean_dict)\n",
    "\t\tfor year, nums in obj.items():\n",
    "\t\t\tif mean_dict[model].get(year) is None:\n",
    "\t\t\t\t# print(f'Initializing {model} {year}')\n",
    "\t\t\t\tres_dict[model][year] = {}\n",
    "\t\t\t\tmean_dict[model][year] = {}\n",
    "\t\t\tdf = pd.DataFrame(nums)\n",
    "\t\t\tstatsd = df.describe()\n",
    "\t\t\tmean_dict[model][year][year] = statsd[0]['mean']\n",
    "\t\t\tres_dict[model][year][year] = nums"
   ],
   "id": "331bca92f751317c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.style.available",
   "id": "f14cf07a6a4c0d45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.style.use('seaborn-v0_8-paper')\n",
    "for model, model_dct in res_dict.items():\n",
    "\tfor eval_year, dct in model_dct.items():\n",
    "\t\ttrain_years = dct.keys()\n",
    "\t\taucs = dct.values()\n",
    "\t\tplt.boxplot(aucs,tick_labels=train_years)\n",
    "\t\tplt.xlabel('Model Training Year')\n",
    "\t\tplt.ylabel('AUC measurements')\n",
    "\t\tplt.title(f'{model} evals for {eval_year}')\n",
    "\t\tplt.show()"
   ],
   "id": "2ae3ada85b40de62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Statistical testing methodology:\n",
    "- Run Shapiro-Wilk test on auc data from each test-train pair\n",
    "- Run Anova on eval years (within model)\n",
    "- Run Anova on eval years (across models?)"
   ],
   "id": "c999274e302d9486"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_nonnormal = 0\n",
    "num_heteroscedastic = 0\n",
    "total_num = 0\n",
    "for model, mod_d in res_dict.items():\n",
    "\tfor eval_year, year_d in mod_d.items():\n",
    "\t\tvariances = []\n",
    "\t\tfor train_year, nums in year_d.items():\n",
    "\t\t\ttest_result = stats.shapiro(nums)\n",
    "\t\t\tvariances.append(stats.describe(nums).variance)\n",
    "\t\t\ttotal_num += 1\n",
    "\t\t\tif test_result.pvalue <= 0.05:\n",
    "\t\t\t\tprint(f\"Normality assumption violated for model {model} trained on {train_year} evaluated on {eval_year}\")\n",
    "\t\t\t\tprint(f'p={test_result.pvalue}')\n",
    "\t\t\t\t# stats.probplot(nums,dist='norm', plot=plt)\n",
    "\t\t\t\t# plt.show()\n",
    "\t\t\t\tnum_nonnormal += 1\n",
    "\t\tif max(variances) / min(variances) >= 2:\n",
    "\t\t\tnum_heteroscedastic += 1\n",
    "\t\t\tprint(f\"Heteroscedasticity detected for {model} eval on {eval_year}\")\n",
    "\t\t\tprint(f'Ratio: {max(variances) / min(variances)}')"
   ],
   "id": "c5db2cdd7c2413c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "num_nonnormal / total_num",
   "id": "a8d62ae264e726b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "num_heteroscedastic / total_num",
   "id": "af017acd434c699c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Shapiro Wilk results\n",
    " Roughly 18% of samples are non-normal w/ alpha of 0.1, down to 11% of samples w/alpha of 0.05. I think using t-tests/ANOVAs to compare samples is warranted.\n",
    "## Heteroscedasticity results\n",
    " ~23% of samples showed heteroscedasticity,"
   ],
   "id": "b5f29ec9b1b4262b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Significance Tests\n",
    "model_test_results = {}\n",
    "for model, model_dct in res_dict.items():\n",
    "\tmodel_test_results[model] = {}\n",
    "\tfor eval_year, year_d in model_dct.items():\n",
    "\t\tif len(year_d.keys()) == 1: # Can't really do anything with only one sample\n",
    "\t\t\tpass\n",
    "\t\telif len(year_d.keys()) == 2: # t-test\n",
    "\t\t\tvals = list(year_d.values())\n",
    "\t\t\tmodel_test_results[model][eval_year] = stats.ttest_ind(vals[0],vals[1],equal_var=False)\n",
    "\t\telif len(year_d.keys()) == 3: # ANOVA\n",
    "\t\t\tvals = list(year_d.values())\n",
    "\t\t\tmodel_test_results[model][eval_year] = stats.f_oneway(vals[0],vals[1],vals[2])\n",
    "\t\telif len(year_d.keys()) == 4: # ANOVA\n",
    "\t\t\tvals = list(year_d.values())\n",
    "\t\t\tmodel_test_results[model][eval_year] = stats.f_oneway(vals[0],vals[1],vals[2],vals[3])\n",
    "\t\telif len(year_d.keys()) == 5: # ANOVA\n",
    "\t\t\tvals = list(year_d.values())\n",
    "\t\t\tmodel_test_results[model][eval_year] = stats.f_oneway(vals[0],vals[1],vals[2],vals[3],vals[4])"
   ],
   "id": "f7fcf55fea2f845a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_test_results",
   "id": "b9f40717f7dbcada",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_test_results['SAKT']",
   "id": "c80952d9cab1adad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93ae2a0c89281bd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Overall rot analysis",
   "id": "23742e20084f1921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "biglist = []\n",
    "for model, mod_dict in res_dict.items():\n",
    "\tfor eval_year, year_dict in mod_dict.items():\n",
    "\t\tfor train_year, nums in year_dict.items():\n",
    "\t\t\tfor i in nums:\n",
    "\t\t\t\tbiglist.append([model,eval_year,train_year,i])"
   ],
   "id": "28a438d53485836c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.DataFrame(biglist,columns=['model','eval_year','train_year','auc'])",
   "id": "f8590f01592d3073",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "eval_years = df.eval_year.unique()\n",
    "\n",
    "for y in eval_years:\n",
    "\tsubset = df[df['eval_year'] == y]\n",
    "\tif y != '19-20':\n",
    "\t\tsns.barplot(x=subset['model'],y=subset['auc'],hue=subset['train_year'])\n",
    "\t\tplt.legend(loc='lower left')\n",
    "\telse:\n",
    "\t\tsns.barplot(x=subset['model'],y=subset['auc'])\n",
    "\tplt.xlabel('KT Model',fontsize=18)\n",
    "\tplt.ylabel('AUC measurements',fontsize=18)\n",
    "\tplt.title(f'Models evaluated on Academic Year {y}',fontsize=20)\n",
    "\t\n",
    "\tplt.show()"
   ],
   "id": "a1cd0af36233f41b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ],
   "id": "17ae6ff5f05ac26c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ols(\"auc ~ C(model) + C(eval_year) + C(train_year) + C(model):C(eval_year):C(train_year)\", data=df).fit(cov_type='hc3')\n",
    "sm.stats.anova_lm(model, typ=2, robust='hc3')"
   ],
   "id": "13981043793cd595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ols(\"auc ~ C(model) + C(eval_year) + C(train_year) + C(model):C(eval_year) +C(train_year):C(eval_year) + C(model):C(train_year)\", data=df).fit(cov_type='hc3')\n",
    "sm.stats.anova_lm(model, typ=2, robust='hc3')"
   ],
   "id": "586133d231013bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ols(\"auc ~ C(model) * C(eval_year) * C(train_year)\", data=df).fit(cov_type='hc3')\n",
    "sm.stats.anova_lm(model, typ=2, robust='hc3')"
   ],
   "id": "8d914f2d671b0f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ols(\"auc ~ C(model) + C(eval_year) + C(train_year) + C(model):C(eval_year) +C(train_year):C(eval_year) + C(model):C(train_year):C(eval_year)\", data=df).fit(cov_type='hc3')\n",
    "sm.stats.anova_lm(model, typ=2, robust='hc3')"
   ],
   "id": "7b47ef89a7ddbd4c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
